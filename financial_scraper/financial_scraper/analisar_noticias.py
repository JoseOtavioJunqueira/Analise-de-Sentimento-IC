import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, BertForSequenceClassification
import io
import re # M√≥dulo de express√µes regulares

# --- 1. CONFIGURA√á√ÉO DOS ARQUIVOS ---
arquivo_json_entrada = 'dados_noticias.json'
arquivo_json_saida = 'noticias_com_sentimento.json'

# --- 2. LEITURA E PARSE ROBUSTO DO JSON ---
print(f"Lendo e extraindo blocos do arquivo '{arquivo_json_entrada}'...")
try:
    with open(arquivo_json_entrada, 'r', encoding='utf-8') as f:
        conteudo_bruto = f.read()

    # --- NOVA ABORDAGEM: ENCONTRAR TODOS OS BLOCOS JSON V√ÅLIDOS ---
    # Encontra todas as ocorr√™ncias de texto que come√ßam com '[' e terminam com ']'
    blocos_json_encontrados = re.findall(r'(\[.*?\])', conteudo_bruto, re.DOTALL)
    
    if not blocos_json_encontrados:
        raise ValueError("Nenhum bloco de dados JSON v√°lido (come√ßando com '[' e terminando com ']') foi encontrado no arquivo.")

    lista_de_dfs = []
    # Itera sobre cada bloco de texto JSON encontrado
    for i, bloco in enumerate(blocos_json_encontrados):
        # Converte cada bloco em um DataFrame e adiciona a uma lista
        df_bloco = pd.read_json(io.StringIO(bloco))
        lista_de_dfs.append(df_bloco)
        print(f"Bloco {i+1} processado com sucesso ({len(df_bloco)} linhas).")

    # Concatena todos os DataFrames da lista em um √∫nico DataFrame
    df = pd.concat(lista_de_dfs, ignore_index=True)
    print("Todos os blocos foram consolidados com sucesso!")

except FileNotFoundError:
    print(f"ERRO: Arquivo '{arquivo_json_entrada}' n√£o encontrado.")
    exit()
except Exception as e:
    print(f"ERRO ao processar o arquivo JSON: {e}")
    exit()

# --- 3. LIMPEZA E PREPARA√á√ÉO DOS DADOS (sem altera√ß√£o) ---
print("Limpando e preparando os textos...")
df['title'] = df['title'].fillna('')
df['content'] = df['content'].fillna('')
df['texto_completo'] = (df['title'].str.strip() + ' ' + df['content'].str.strip()).str.strip()

linhas_antes = len(df)
df = df[df['texto_completo'] != ''].reset_index(drop=True)
linhas_depois = len(df)
print(f"{linhas_antes - linhas_depois} linhas vazias foram removidas.")


# --- 4. L√ìGICA DO MODELO DE SENTIMENTO (sem altera√ß√£o) ---
pred_mapper = {0: "POSITIVE", 1: "NEGATIVE", 2: "NEUTRAL"}

print("Carregando o modelo FinBERT... (Isso pode demorar um pouco)")
tokenizer = AutoTokenizer.from_pretrained("lucas-leme/FinBERT-PT-BR")
model = BertForSequenceClassification.from_pretrained("lucas-leme/FinBERT-PT-BR")

def prever_sentimento(texto):
    if not isinstance(texto, str) or not texto:
        return "TEXTO_INVALIDO"
    inputs = tokenizer(texto, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    prediction = np.argmax(logits.numpy())
    return pred_mapper[prediction]

# --- 5. APLICA√á√ÉO DO MODELO (sem altera√ß√£o) ---
print(f"Iniciando a classifica√ß√£o de sentimento em {len(df)} not√≠cias...")
df['sentimento_previsto'] = df['texto_completo'].apply(prever_sentimento)

# --- 6. SALVAR O RESULTADO FINAL (sem altera√ß√£o) ---
print(f"Salvando os resultados em '{arquivo_json_saida}'...")
df.to_json(
    arquivo_json_saida,
    orient='records',
    indent=4,
    force_ascii=False
)

print("\nüöÄ Processo conclu√≠do com sucesso!")
print(f"‚úÖ O arquivo '{arquivo_json_saida}' foi gerado corretamente.")