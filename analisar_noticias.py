import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, BertForSequenceClassification
import io
import re  # M√≥dulo de express√µes regulares
import os  # Para verificar se o arquivo existe
import dateparser # Nova biblioteca para normalizar datas
from datetime import datetime

# --- 1. CONFIGURA√á√ÉO DOS ARQUIVOS ---
arquivo_json_entrada = r'C:\Users\Jos√© Ot√°vio\Documents\GitHub\Analise-de-Sentimento-IC\financial_scraper\financial_news.json'
arquivo_json_saida = 'noticias_com_sentimento.json'

# --- 2. NOVAS FUN√á√ïES DE AJUDA ---

def normalizar_data(data_str):
    """
    Tenta converter v√°rios formatos de string OU N√öMERO de data para um 
    formato padronizado (ISO 8601).
    Agora trata timestamps de 10 (segundos) e 13 (milissegundos) d√≠gitos.
    """
    if not data_str:
        return None

    # Converte para string para checagem, caso seja int/float
    data_como_str = str(data_str) 
    
    # --- NOVO: Tratamento de Timestamps (Unix) ---
    # Se for num√©rico (int ou string de d√≠gitos)
    if data_como_str.isdigit():
        try:
            num_data = int(data_como_str)
            num_digitos = len(data_como_str)

            timestamp_sec = None
            if num_digitos == 13: # Provavelmente milissegundos
                timestamp_sec = num_data / 1000.0
            elif num_digitos == 10: # Provavelmente segundos
                timestamp_sec = float(num_data) # Converte para float para consist√™ncia
            
            if timestamp_sec is not None:
                # Verifica√ß√£o de sanidade: garante que o timestamp esteja 
                # em um intervalo razo√°vel (ex: entre 2000 e 2050)
                # 946684800 = 2000-01-01
                # 2524608000 = 2050-01-01
                if 946684800 < timestamp_sec < 2524608000:
                     data_obj = datetime.fromtimestamp(timestamp_sec)
                     return data_obj.isoformat()
                else:
                    # √â um n√∫mero, mas fora do intervalo de timestamp esperado
                    # Deixa o dateparser tentar, mas √© improv√°vel que funcione
                    pass

        except Exception:
            # Falhou a convers√£o num√©rica, deixa o dateparser tentar abaixo
            pass 
    
    # --- L√≥gica Anterior (Dateparser) ---
    # Se n√£o for um timestamp v√°lido ou for uma string (ex: "6 de maio...")
    try:
        # Usamos 'pt' (portugu√™s) como l√≠ngua priorit√°ria
        # O str(data_str) garante que o dateparser receba uma string
        data_obj = dateparser.parse(str(data_str), languages=['pt']) 
        if data_obj:
            # Retorna a data no formato padr√£o ISO (YYYY-MM-DDTHH:MM:SS)
            return data_obj.isoformat()
        return None
    except Exception:
        # Falha final, n√£o foi poss√≠vel converter
        return None
    
def carregar_noticias_existentes(arquivo_saida):
    """
    Carrega as not√≠cias j√° processadas do arquivo de sa√≠da.
    Retorna um DataFrame com os dados e um set() com os t√≠tulos para deduplica√ß√£o.
    """
    if not os.path.exists(arquivo_saida):
        print(f"Arquivo '{arquivo_saida}' n√£o encontrado. Um novo ser√° criado.")
        return pd.DataFrame(), set()
        
    try:
        df_existente = pd.read_json(arquivo_saida, orient='records')
        # Cria um conjunto de t√≠tulos para verifica√ß√£o r√°pida de duplicatas
        titulos_existentes = set(df_existente['title'].dropna())
        print(f"Encontradas {len(df_existente)} not√≠cias j√° processadas.")
        return df_existente, titulos_existentes
    except Exception as e:
        print(f"AVISO: N√£o foi poss√≠vel ler o arquivo '{arquivo_saida}'. Ele pode estar vazio ou corrompido. Come√ßando do zero. Erro: {e}")
        return pd.DataFrame(), set()

def ler_novas_noticias(arquivo_entrada):
    """
    L√™ o arquivo de entrada bruto, tratando m√∫ltiplos blocos JSON.
    """
    print(f"Lendo e extraindo blocos do arquivo '{arquivo_entrada}'...")
    try:
        with open(arquivo_entrada, 'r', encoding='utf-8') as f:
            conteudo_bruto = f.read()

        if not conteudo_bruto.strip():
            print("Arquivo de entrada est√° vazio. Nada a processar.")
            return None

        # Encontra todas as ocorr√™ncias de texto que come√ßam com '[' e terminam com ']'
        blocos_json_encontrados = re.findall(r'(\[.*?\])', conteudo_bruto, re.DOTALL)
        
        if not blocos_json_encontrados:
            print("Nenhum bloco de dados JSON v√°lido (come√ßando com '[' e terminando com ']') foi encontrado no arquivo de entrada.")
            return None

        lista_de_dfs = []
        for i, bloco in enumerate(blocos_json_encontrados):
            df_bloco = pd.read_json(io.StringIO(bloco))
            lista_de_dfs.append(df_bloco)
        
        df_novas = pd.concat(lista_de_dfs, ignore_index=True)
        print(f"Arquivo de entrada consolidado com sucesso ({len(df_novas)} not√≠cias brutas).")
        return df_novas

    except FileNotFoundError:
        print(f"ERRO: Arquivo '{arquivo_entrada}' n√£o encontrado.")
        return None
    except Exception as e:
        print(f"ERRO ao processar o arquivo de entrada JSON: {e}")
        return None

def limpar_arquivo_entrada(arquivo_entrada):
    """
    Limpa o arquivo de entrada ap√≥s o processamento bem-sucedido.
    """
    try:
        # Escreve uma lista vazia para manter o arquivo como um JSON v√°lido (opcional)
        with open(arquivo_entrada, 'w', encoding='utf-8') as f:
            f.write("[]") 
        print(f"Arquivo de entrada '{arquivo_entrada}' foi limpo.")
    except Exception as e:
        print(f"ERRO ao limpar o arquivo de entrada '{arquivo_entrada}': {e}")


# --- 3. IN√çCIO DO PIPELINE DE PROCESSAMENTO ---

# 3.1. Carregar dados existentes para evitar duplicatas
df_existente, titulos_existentes = carregar_noticias_existentes(arquivo_json_saida)

# 3.2. Ler as novas not√≠cias do arquivo de entrada
df_novas_noticias = ler_novas_noticias(arquivo_json_entrada)

# Se n√£o houver novas not√≠cias, encerra o script
if df_novas_noticias is None or df_novas_noticias.empty:
    print("Nenhuma not√≠cia nova encontrada para processar. Encerrando.")
    exit()

# 3.3. **NOVO: Deduplica√ß√£o**
# Filtra o DataFrame de novas not√≠cias, mantendo apenas aquelas
# cujo 'title' N√ÉO EST√Å no conjunto de 'titulos_existentes'.
df_para_processar = df_novas_noticias[~df_novas_noticias['title'].isin(titulos_existentes)].reset_index(drop=True)

if df_para_processar.empty:
    print("Todas as not√≠cias do arquivo de entrada j√° foram processadas anteriormente.")
    # Limpamos o arquivo de entrada mesmo assim, pois j√° foram processadas
    limpar_arquivo_entrada(arquivo_json_entrada)
    print("Encerrando.")
    exit()

print(f"Encontradas {len(df_para_processar)} not√≠cias realmente novas para processar.")

# --- 4. LIMPEZA E PREPARA√á√ÉO DOS DADOS (Agora em 'df_para_processar') ---
print("Limpando e preparando os textos...")
df_para_processar['title'] = df_para_processar['title'].fillna('')
df_para_processar['content'] = df_para_processar['content'].fillna('')
df_para_processar['texto_completo'] = (df_para_processar['title'].str.strip() + ' ' + df_para_processar['content'].str.strip()).str.strip()

linhas_antes = len(df_para_processar)
df_para_processar = df_para_processar[df_para_processar['texto_completo'] != ''].reset_index(drop=True)
linhas_depois = len(df_para_processar)
print(f"{linhas_antes - linhas_depois} linhas vazias foram removidas.")

# 4.1. **NOVO: Normaliza√ß√£o da Data**
# Vamos supor que sua coluna de data se chama 'date'. 
# Se o nome for outro (ex: 'data', 'timestamp'), apenas troque 'date' abaixo.
if 'date' in df_para_processar.columns:
    print("Normalizando datas...")
    df_para_processar['data_normalizada'] = df_para_processar['date'].apply(normalizar_data)
else:
    print("AVISO: Coluna 'date' n√£o encontrada. Pulando normaliza√ß√£o de data.")


# --- 5. L√ìGICA DO MODELO DE SENTIMENTO ---
pred_mapper = {0: "POSITIVE", 1: "NEGATIVE", 2: "NEUTRAL"}

print("Carregando o modelo FinBERT... (Isso pode demorar um pouco)")
tokenizer = AutoTokenizer.from_pretrained("lucas-leme/FinBERT-PT-BR")
model = BertForSequenceClassification.from_pretrained("lucas-leme/FinBERT-PT-BR")

def prever_sentimento(texto):
    if not isinstance(texto, str) or not texto:
        return "TEXTO_INVALIDO"
    inputs = tokenizer(texto, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    prediction = np.argmax(logits.numpy())
    return pred_mapper[prediction]

# --- 6. APLICA√á√ÉO DO MODELO ---
print(f"Iniciando a classifica√ß√£o de sentimento em {len(df_para_processar)} not√≠cias...")
# Usamos .copy() para evitar o SettingWithCopyWarning
df_processado = df_para_processar.copy()
df_processado['sentimento_previsto'] = df_processado['texto_completo'].apply(prever_sentimento)


# --- 7. **NOVO: COMBINAR E SALVAR O RESULTADO FINAL** ---
print("Combinando not√≠cias existentes com as novas processadas...")
# Concatena o DataFrame antigo com o novo DataFrame j√° processado
df_final_completo = pd.concat([df_existente, df_processado], ignore_index=True)

print(f"Salvando {len(df_final_completo)} not√≠cias no total em '{arquivo_json_saida}'...")
try:
    df_final_completo.to_json(
        arquivo_json_saida,
        orient='records',
        indent=4,
        force_ascii=False
    )
    
    print("\nüöÄ Processo conclu√≠do com sucesso!")
    print(f"‚úÖ O arquivo '{arquivo_json_saida}' foi atualizado.")

    # 7.1. **NOVO: Limpar arquivo de entrada**
    # Somente limpa o arquivo de entrada se o salvamento foi bem-sucedido
    limpar_arquivo_entrada(arquivo_json_entrada)

except Exception as e:
    print(f"\nERRO CR√çTICO ao salvar o arquivo final: {e}")
    print("ATEN√á√ÉO: O arquivo de entrada N√ÉO foi limpo para evitar perda de dados.")